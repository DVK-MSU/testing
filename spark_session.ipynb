{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DVK-MSU/testing/blob/main/spark_session_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEDZZ02HvsuW",
        "outputId": "a5809f0c-872a-425e-87d2-089135e4a60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#Функция для передачи результата sql-запроса в pandas DataFrame.\n",
        "#Напиши select = \"\"\" SELECT * FROM table\"\"\" и вызови функцию sql(select)\n",
        "def sql(select):\n",
        "    SQLdf = spark.sql(select)\n",
        "    return SQLdf.toPandas()\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "from google.colab import data_table\n",
        "data_table.disable_dataframe_formatter()\n",
        "\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "2qE0St6w57Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reheadering_ETL():\n",
        "    #extraction data from csv\n",
        "    path = \"/content/drive/MyDrive/Spark_training/california_housing_train.csv\"\n",
        "    sc = spark.read.csv(path)\n",
        "    sc.createOrReplaceTempView(\"df\")\n",
        "    #transformation data (reheadering columns)\n",
        "    sc_pandas = sc.toPandas()\n",
        "    columns_name = dict(zip(sc_pandas.columns.to_list(),sc_pandas.iloc[0].to_list()))\n",
        "    sc_pandas.rename(columns=columns_name, inplace=True)\n",
        "    sc_pandas.drop([0], inplace=True)\n",
        "    sc = spark.createDataFrame(sc_pandas)\n",
        "    #loading data to json\n",
        "    sc.write.json('/content/drive/MyDrive/Spark_training/output', mode='overwrite')\n",
        "reheadering_ETL()\n"
      ],
      "metadata": {
        "id": "vH5JnlkO3_Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.json('/content/drive/MyDrive/Spark_training/output').show() # output data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CWlMtJJcZk5",
        "outputId": "eee8d824-e64d-430c-b58f-f154f3a9df03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+---------+-----------+------------------+-------------+-----------+--------------+-----------+\n",
            "| households|housing_median_age| latitude|  longitude|median_house_value|median_income| population|total_bedrooms|total_rooms|\n",
            "+-----------+------------------+---------+-----------+------------------+-------------+-----------+--------------+-----------+\n",
            "| 472.000000|         15.000000|34.190000|-114.310000|      66900.000000|     1.493600|1015.000000|   1283.000000|5612.000000|\n",
            "| 463.000000|         19.000000|34.400000|-114.470000|      80100.000000|     1.820000|1129.000000|   1901.000000|7650.000000|\n",
            "| 117.000000|         17.000000|33.690000|-114.560000|      85700.000000|     1.650900| 333.000000|    174.000000| 720.000000|\n",
            "| 226.000000|         14.000000|33.640000|-114.570000|      73400.000000|     3.191700| 515.000000|    337.000000|1501.000000|\n",
            "| 262.000000|         20.000000|33.570000|-114.570000|      65500.000000|     1.925000| 624.000000|    326.000000|1454.000000|\n",
            "| 239.000000|         29.000000|33.630000|-114.580000|      74000.000000|     3.343800| 671.000000|    236.000000|1387.000000|\n",
            "| 633.000000|         25.000000|33.610000|-114.580000|      82400.000000|     2.676800|1841.000000|    680.000000|2907.000000|\n",
            "| 158.000000|         41.000000|34.830000|-114.590000|      48500.000000|     1.708300| 375.000000|    168.000000| 812.000000|\n",
            "|1056.000000|         34.000000|33.610000|-114.590000|      58400.000000|     2.178200|3134.000000|   1175.000000|4789.000000|\n",
            "| 271.000000|         46.000000|34.830000|-114.600000|      48100.000000|     2.190800| 787.000000|    309.000000|1497.000000|\n",
            "| 824.000000|         16.000000|33.620000|-114.600000|      86500.000000|     2.679700|2434.000000|    801.000000|3741.000000|\n",
            "| 437.000000|         21.000000|33.600000|-114.600000|      62000.000000|     1.625000|1182.000000|    483.000000|1988.000000|\n",
            "| 211.000000|         48.000000|34.840000|-114.610000|      48600.000000|     2.157100| 580.000000|    248.000000|1291.000000|\n",
            "| 479.000000|         31.000000|34.830000|-114.610000|      70400.000000|     3.212000|1346.000000|    464.000000|2478.000000|\n",
            "| 300.000000|         15.000000|32.760000|-114.630000|      45000.000000|     0.858500| 949.000000|    378.000000|1448.000000|\n",
            "| 401.000000|         17.000000|34.890000|-114.650000|      69100.000000|     1.699100|1005.000000|    587.000000|2556.000000|\n",
            "| 256.000000|         28.000000|33.600000|-114.650000|      94900.000000|     2.965300| 666.000000|    322.000000|1678.000000|\n",
            "|  27.000000|         21.000000|32.790000|-114.650000|      25000.000000|     0.857100|  64.000000|     33.000000|  44.000000|\n",
            "| 320.000000|         17.000000|32.740000|-114.660000|      44000.000000|     1.204900| 775.000000|    386.000000|1388.000000|\n",
            "|  15.000000|         17.000000|33.920000|-114.670000|      27500.000000|     1.265600|  29.000000|     24.000000|  97.000000|\n",
            "+-----------+------------------+---------+-----------+------------------+-------------+-----------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc_renamed = spark.read.json('/content/drive/MyDrive/Spark_training/output')\n",
        "sc_renamed.dtypes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To0_Lu--qRHg",
        "outputId": "20591023-ed9b-4abb-8efd-9fe0cfcbc16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sucsess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def FloatType_Transform(sc_renamed):\n",
        "    #Extract\n",
        "    sc_renamed = spark.read.json('/content/drive/MyDrive/Spark_training/output')\n",
        "    #Transform\n",
        "    from pyspark.sql.functions import col\n",
        "    return sc_renamed.select([col(col_name).cast('float') for col_name in sc_renamed.columns])\n",
        "sc_renamed= sc_renamed.transform(FloatType_Transform)\n",
        "sc_renamed.write.json('/content/drive/MyDrive/Spark_training/output/float_type_transform', mode='overwrite')"
      ],
      "metadata": {
        "id": "O-KoZ8pSswup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.json('/content/drive/MyDrive/Spark_training/output/float_type_transform').dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0kb7buuyOyV",
        "outputId": "8d6686e9-358b-4792-89de-30a7a8825bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('households', 'double'),\n",
              " ('housing_median_age', 'double'),\n",
              " ('latitude', 'double'),\n",
              " ('longitude', 'double'),\n",
              " ('median_house_value', 'double'),\n",
              " ('median_income', 'double'),\n",
              " ('population', 'double'),\n",
              " ('total_bedrooms', 'double'),\n",
              " ('total_rooms', 'double')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEWf8ZyJr09X",
        "outputId": "087f9cbb-336e-475c-ebf5-41f543118381"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}